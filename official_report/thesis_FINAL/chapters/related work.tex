There are other notions of conformance other than fitness. \cite{buijs2012role} considers precision, generalization and simplicity in addition to fitness. Precision measures how well the model is restricted to just the actual observed behavior, i.e. testing if the model allows \emph{more} than what happened in reality. This is explored extensively in \cite{munoz2016conformance}. Generalization is characterized by the chance that a the next incoming trace outside the training set also fits on the model. Simplicity measures the complexity and redundancy of the model which is important for human readability.

For fitness specifically, several different approaches are presented in \cite{van2016process}.
Footprint based conformance compares the possible behavior of log and model through a directly follows abstraction. This measure takes into account fitness as well as precision, so it is not very specific. With token-based replay conformance checking, the number of produced, consumed, missing and remaining tokens are counted. This measures fitness but is influenced by the simplicity of the model because models with redundant places and the same behavior achieve a higher value.
Lastly, alignment based conformance \cite{adriansyah2015measuring} is the state of the art approach to obtain stable fitness values.
This is also the base for performance analysis. The events in the log are projected onto the synchronous moves of the alignment and used for calculating sojourn times and activity execution times.
Our approach combines alignments with place-local token-based replay.

There are also various approaches dealing with a localization over control-flow.
One application is decomposition of the process model. In \cite{van2013decomposing} a generic concept to decompose discovery and conformance checking into smaller problems to increase efficiency is introduced. The log is projected onto overlapping sets of activities. This way, the results can be recombined afterwards. Since most algorithms for discovery and conformance checking are linear in the number of traces and exponential in the number of activities, an exponential speedup can be achieved even on a single core machine. A framework supporting multiple discovery algorithms has been implemented in \cite{verbeek2017divide}. For decomposed conformance checking, the implementation in \cite{lee2018recomposing} provides exact alignment based fitness values and strong guarantees for time-bound approximations by using re-composition to merge partial results.
A novel approach also using a localization to places in discovery is \cite{van2018discovering}. Here, the model discovery is entirely decomposed to individual places. Places are viewed as a pair of ingoing arcs and outgoing arcs to transitions. Based on these sets, a partial order over possible places is defined. It ranks the ability of a place to fill with tokens. A light place has few input arcs and many output arcs. A heavy place has many input arcs and fewer output arcs. A variety of quality measures for candidate places is defined and monotonicity over this partial order is proven. Based on this, many ways of pruning the search space are demonstrated in a generic way. Lastly, an application to efficient conformance checking is proposed.

Localization over the time dimension is used in streaming model discovery approaches which try to deal with concept drift. In \cite{bose2011handling}, different features like the causal footprint of the log, which is derived from the directly-follows relation of activities, are measured over time and used to detect change points with statistical hypothesis tests. Then, a model is discovered for each section between change points. \cite{manoj15capturing} also uses both localization over control-flow and time in the log to detect concept drift via statistical testing.

Considering multiple dimensions is also possible by clustering the log along chosen dimensions at first and then using conventional tools on the sublogs to obtain fine grained results. This is done in \cite{hompes2015detecting} with comparative trace clustering. After clustering, the models for the sublogs can provide more insight into seemingly unstructured processes. \cite{de2016general} proposes a general framework for case clustering while allowing for additional, user-defined, features to be added to events. This way, even more perspectives on the log may be considered.

Approaches combining several perspectives for diagnosing log and model have also been proposed. The multi-perspective process explorer \cite{mannhardt2015multi} building onto \cite{mannhardt2016balanced}, provides views for conformance, performance and data-flow. The alignments in \cite{mannhardt2016balanced} also take into account multiple dimensions like data, making each dimension a first-class citizen of the alignment, to allow more balanced conformance checking that is not dominated by control-flow.

In contrast to existing conformance and performance related approaches, we use localization over time in addition to control-flow to allow users to quickly compare and explore detailed metrics. Especially the timeseries functionality is an improvement over the state-of-the-art. Additionally, we provide a process context perspective and the opportunity for clustering and correlation with the exportable dataset.